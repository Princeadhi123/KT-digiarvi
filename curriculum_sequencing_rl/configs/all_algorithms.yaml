environment:
  data_path: "../preprocessed_kt_data.csv"
  reward_correct_w: 1.0
  reward_score_w: 0.0
  seed: 789

models: ["ql", "dqn", "a2c", "a3c", "ppo"]

# Q-Learning configuration
q_learning:
  epochs: 5
  alpha: 0.2
  gamma: 0.9
  eps_start: 0.3
  eps_end: 0.0
  eps_decay_epochs: 3

# DQN configuration
dqn:
  episodes: 100
  learning_rate: 0.001
  gamma: 0.99
  batch_size: 128
  buffer_size: 20000
  hidden_dim: 128
  eps_start: 1.0
  eps_end: 0.05
  eps_decay_steps: 10000

# A2C configuration
a2c:
  episodes: 100
  learning_rate: 0.001
  entropy_coef: 0.01
  value_coef: 0.5
  bc_warmup: 2
  bc_weight: 1.0
  batch_episodes: 4

# A3C configuration
a3c:
  episodes: 100
  learning_rate: 0.001
  entropy_coef: 0.01
  value_coef: 0.5
  gae_lambda: 0.95
  bc_warmup: 2
  bc_weight: 1.0
  rollouts: 4

# PPO configuration
ppo:
  episodes: 100
  learning_rate: 0.0003
  clip_eps: 0.2
  epochs: 4
  batch_episodes: 8
  minibatch_size: 1024
  entropy_coef: 0.01
  value_coef: 0.5
  gae_lambda: 0.95
  bc_warmup: 2
  bc_weight: 1.0

# Evaluation settings
eval_episodes: 300
include_chance: true
include_trivial: true
include_markov: true
metrics_csv: "all_algorithms_results.csv"
