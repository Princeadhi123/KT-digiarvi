environment:
  data_path: "preprocessed_kt_data.csv"
  reward_correct_w: 0.0
  reward_score_w: 1.0
  # Hybrid weights
  hybrid_base_w: 1.0
  hybrid_mastery_w: 1.0
  hybrid_motivation_w: 1.0
  # Multi-objective shaping weights
  rew_improve_w: 0.5
  rew_deficit_w: 0.5
  rew_spacing_w: 0.2
  rew_diversity_w: 0.2
  rew_challenge_w: 0.2
  # Shaping hyperparameters
  ema_alpha: 0.3
  need_threshold: 0.6
  spacing_window: 5
  diversity_recent_k: 5
  challenge_target: 0.7
  challenge_band: 0.4
  invalid_penalty: -0.1
  seed: 123

# Run a fast multi-model smoke to validate cross-model normalization
models: ["ql", "dqn", "a2c", "a3c", "ppo", "sarl"]

# Q-Learning configuration
q_learning:
  epochs: 200
  alpha: 0.2
  gamma: 0.9
  eps_start: 0.3
  eps_end: 0.0
  eps_decay_epochs: 3
  select_best_on_val: true
  val_episodes: 300

# DQN configuration (keys align to DQNConfig dataclass)
dqn:
  episodes: 200
  lr: 0.0002
  gamma: 0.997
  batch_size: 512
  buffer_size: 120000
  hidden_dim: 512
  eps_start: 1.0
  eps_end: 0.01
  eps_decay_steps: 20000
  target_tau: 0.01
  target_update_interval: 1
  select_best_on_val: true
  val_episodes: 500

# A2C configuration (keys align to A2CConfig/PolicyGradientConfig)
a2c:
  episodes: 200
  lr: 0.0003
  entropy_coef: 0.02
  value_coef: 0.5
  bc_warmup_epochs: 8
  bc_weight: 2.0
  batch_episodes: 8

# A3C configuration (keys align to A3CConfig)
a3c:
  episodes: 200
  lr: 0.0003
  entropy_coef: 0.02
  value_coef: 0.5
  gae_lambda: 0.95
  bc_warmup_epochs: 8
  bc_weight: 2.0
  rollouts_per_update: 8

# PPO configuration (keys align to PPOConfig)
ppo:
  episodes: 200
  lr: 0.0003
  clip_eps: 0.2
  ppo_epochs: 8
  batch_episodes: 16
  minibatch_size: 1024
  entropy_coef: 0.02
  value_coef: 0.5
  gae_lambda: 0.95
  bc_warmup_epochs: 8
  bc_weight: 2.0

# SARL (Self-Adaptive RL) configuration (keys align to SARLDQNConfig)
sarl:
  episodes: 300
  lr: 0.0003
  gamma: 0.997
  batch_size: 512
  buffer_size: 120000
  hidden_dim: 512
  eps_start: 1.0
  eps_end: 0.01
  eps_decay_steps: 20000
  target_tau: 0.01
  target_update_interval: 1
  select_best_on_val: true
  val_episodes: 500
  eval_episodes: 300
  train_max_steps_per_episode: 12
  eval_max_steps_per_episode: 12
  val_max_steps_per_episode: 12
  adapt_interval: 3
  adapt_regret: true
  regret_ratio_target: 0.15
  regret_ratio_tolerance: 0.02
  regret_epsilon_up_step: 0.05
  regret_epsilon_down_step: 0.02
  regret_base_tilt_step: 0.1
  regret_invalid_penalty_step: 0.02
  hybrid_base_target_share: 0.5
  hybrid_mastery_target_share: 0.25
  hybrid_motivation_target_share: 0.25
  weight_lr: 0.3
  weight_min: 0.1
  weight_max: 3.0
  share_tolerance: 0.05
  invalid_rate_hi: 0.15
  invalid_rate_lo: 0.02
  epsilon_up_step: 0.05
  epsilon_down_step: 0.02
  eps_min: 0.01
  eps_max: 1.0
  lr_patience: 5
  lr_reduce_factor: 0.5
  min_lr: 1e-5
  challenge_increase_threshold: 0.85
  challenge_decrease_threshold: 0.55
  challenge_target_step: 0.05
  challenge_target_min: 0.4
  challenge_target_max: 0.9
  challenge_band_step: -0.05
  challenge_band_min: 0.2
  challenge_band_max: 0.6

# Global eval settings
include_chance: false
include_trivial: false
include_markov: false

# Multi-seed aggregation
seeds: [123, 456, 789]

# Enable scalability/adaptability evaluations
evaluate_scalability: true
scalability_small_fraction: 0.5

evaluate_adaptability: true
adapt_pre_episodes: 80
adapt_post_episodes: 80
# Example post-change: slightly higher challenge
adapt_post_challenge_target: 0.8
adapt_post_challenge_band: 0.3

# Output CSV
metrics_csv: "all_models.csv"
