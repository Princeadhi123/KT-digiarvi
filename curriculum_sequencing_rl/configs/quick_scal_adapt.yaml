environment:
  data_path: "preprocessed_kt_data.csv"
  reward_correct_w: 0.0
  reward_score_w: 1.0
  # Hybrid weights
  hybrid_base_w: 1.0
  hybrid_mastery_w: 1.0
  hybrid_motivation_w: 1.0
  # Multi-objective shaping weights
  rew_improve_w: 0.35
  rew_deficit_w: 0.35
  rew_spacing_w: 0.10
  rew_diversity_w: 0.05
  rew_challenge_w: 0.15
  # Shaping hyperparameters
  ema_alpha: 0.3
  need_threshold: 0.6
  spacing_window: 16
  diversity_recent_k: 10
  challenge_target: 0.7
  challenge_band: 0.4
  invalid_penalty: -0.1
  seed: 123

# Run a fast smoke with Q-Learning across three seeds; add other models later
models: ["ql"]

# Q-Learning configuration
q_learning:
  epochs: 5
  alpha: 0.2
  gamma: 0.9
  eps_start: 0.3
  eps_end: 0.0
  eps_decay_epochs: 3
  select_best_on_val: true
  val_episodes: 300

# Global eval settings
# eval_episodes removed here; defaults live in per-model configs (300). Use CLI --eval_episodes to override globally.
include_chance: true
include_trivial: true
include_markov: true

# Multi-seed aggregation
seeds: [123, 456, 789]

# Enable scalability/adaptability evaluations
evaluate_scalability: true
scalability_small_fraction: 0.5

evaluate_adaptability: true
adapt_pre_episodes: 100
adapt_post_episodes: 100
# Example post-change: slightly higher challenge
adapt_post_challenge_target: 0.8
adapt_post_challenge_band: 0.3

# Output CSV (created in current working directory)
metrics_csv: "scal_adapt_results.csv"
